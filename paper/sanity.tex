\documentclass[10pt,a4paper]{article}
\usepackage{graphicx,amssymb}
\usepackage{authblk}
\usepackage{fullpage}

%\usepackage{geometry}
%\geometry{lmargin=0.3in,rmargin=0.3in,tmargin=0in,bmargin=0.1in}

%\textwidth=18cm \hoffset=-0.6cm
%\textheight=28cm \voffset=-1cm


\pagestyle{empty}

\date{}

\def\keywords#1{\begin{center}{\bf Keywords}\\{#1}\end{center}}

\def\titulo#1{\title{#1}}
\def\autores#1{\author{#1}}

\begin{document}

\titulo{On Genomic Repeats and Reproducibility}

\author[1]{Can Firtina}
\author[1]{Can Alkan}
\affil[1]{\small Dept. of Computer Engineering, Bilkent University, 06800 Ankara, Turkey}
%\renewcommand\Authands{ and }

\maketitle
\thispagestyle{empty}

% The abstract

\begin{abstract}

\keywords{genome analysis, reproducibility, repeats}
\end{abstract}

\section{Introduction}
The advancements in high throughput sequencing (HTS) technologies have increased the demand on producing genome sequence data for many research questions, and prompted pilot projects to test its
power in clinical settings~\cite{Biesecker2009}. Any ``medical test'' to be reliably used in the clinic has to be proven to be both accurate and reproducible.
However, the fast-evolving nature of HTS technologies make it difficult to generate essentially same data using the same DNA resources. 

We recently showed that resequencing the same DNA library 
with the same model HTS instrument twice and analyzing the data with the same algorithms leads to different variation call sets~\cite{Kavak2015}. There may be multiple reasons for this effect,
such as degredation of DNA between two sequencing experiments, signal processing and base calling errors during sequencing, or different GC biases introduced while making sequencing libraries from the
same DNA~\cite{Kavak2015}. 

Aside from the potential problems in the ``wet lab'' side as described above~\cite{Kavak2015}, 
there may be additional complications in the ``dry lab'' analysis due to alignment errors and ambiguities due to genomic repeats.
Approximately half of the human genome consists of repeats, which cause ambiguity in read mapping when the read length is short. On the average, a 100 bp read generated by the Illumina platform may align to hundreds of genome locations with similar edit distance. 
The BWA~\cite{Li2009a} mapper's  approach to handle such ambiguity is randomly selecting one location, and assigning the mapping quality to zero to inform the variant calling algorithms that the alignment may not be accurate. 

Although many algorithms were developed for HTS data analysis, a handful of computational pipelines from mapping to variant calling may be considered ``standard'' as they are commonly used in large-scale genome projects such as the 1000 Genomes Project~\cite{1000GP,1000GP2012}. For example, to discover and genotype variants using Illumina data, first the reads are mapped to reference genome assembly using BWA~\cite{Li2009a} or Bowtie~\cite{Langmead2009}, then the alignment files are processed using SAMtools~\cite{Li2009b} and Picard~\cite{picard}, and finally the single nucleotide polymophisms (SNP) and indels are predicted and filtered using GATK~\cite{DePristo2011}, Platypus~\cite{Rimmer2014}, or Freebayes~\cite{Garrison2012}. 
Structural variation (SV) discovery is even more challenging as exemplified by the 1000 Genomes Project~\cite{1000GP,1000GP2012,Mills2011}, where more than 20 algorithms were used to characterize SVs.
Recently, the Genome in a Bottle Project~\cite{Zook2014} was started to set standards for accurate HTS data analysis for both research and clinical uses by addressing the differences in performances (i.e. accuracy, sensitivity, specificity, etc.) of different algorithms and different sequencing platforms
 to find a ``best practices'' approach.

In this study, we investigated whether some of the commonly used variant discovery algorithms
make use of this mapping quality information, and how they react to genomic repeats.
Briefly, 
we aligned two whole genome shotgun (WGS) datasets with one low (5X) and one high (45X) coverage genome
sequenced as part of the 1000 Genomes Project~\cite{1000GP2012} to the human reference genome (GRCh37) {\bf twice} with the same parameters. 
To test the effects of random placements 
we shuffled the order of reads in the second mapping experiment to make sure that the same random numbers are not used for the same reads. 
In a small scale test, we did not observe any differences
in the alignment files when we used other aligners such as mrFAST~\cite{Alkan2009,Xin2013}, Bowtie~\cite{Langmead2009}, and RazerS~\cite{Weese2012}; however, 
BWA reported different map locations to repetitive regions ($\sim$2.8\% of reads) as expected due to its random placement strategy.
We then generated two single nucleotide polymorphism (SNP) and indel callsets each using GATK~\cite{DePristo2011} HaplotypeCaller, GATK UnifiedGenotyper, Freebayes~\cite{Garrison2012}, Platypus~\cite{Rimmer2014}, and SAMTools~\cite{Li2009b}, and structural variation (SV) callsets using DELLY~\cite{Rausch2012} and LUMPY~\cite{Layer2014} from each genome.

We observed substantial differences in the callsets generated by all of the variant discovery tools we tested. GATK's HaplotypeCaller showed a discrepancy of 0.4\% to 1.1\% , 
where UnifiedGenotyper showed the highest number of different calls between two alignments of the same data set (up to 12.76\%). 
Our results raise questions about reproducibility and accuracy of several commonly used genomic variation discovery tools. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Methods}

\subsection{Data acquisition}
\subsection{Read mapping}
\subsection{SNPs and indels}
\subsection{Structural variation}
\subsection{Variant annotation}
\subsection{Code and script availability}

\section{Results}

As expected, 72 to 80\% of the discrepant calls were found within common repeats. However, we also observed 165 to 4,397 SNVs 
that were called within one alignment but not the other that map to coding exons. 
Furthermore, 691 of the 4397 (15.7\%) discrepant exonic SNVs predicted by GATK UnifiedGenotyper,  did not intersect with any common repeats or segmental duplications.
Freebayes, Platypus, and SAMtools predictions were more reproducable, as $>$98.5\% of the calls were identical.
DELLY also predicted different calls: $\sim$3\% of deletion, $\sim$4\% of tandem duplication, $\sim$6\% of inversion, and $\sim$3.6\% of translocation calls were spefic to a single alignment (i.e. BAM file), and $>$91\% of
these differences intersected with common repeats. More interestingly, when we ran GATK's both HaplotypeCaller and UnifiedGenotyper on the {\bf same} BAM file twice, we observed similar differences. Other tools produced no discrepancies.

As expected, 
Interestingly, although BWA reported zero mapping qualities for 
most of the discrepant read mappings ($\sim$94\%), it also assigned high MAPQ values ($\geq$30) for a fraction of them ($\sim$1.75\%). 


\section{Discussion}

The differences in callsets we observed in this study 
may have similar sensitivity and specificity. It is expected to
have differences between different algorithms and/or parameters, but
obtaining different results should not be due to the order of {\it independently generated} reads in the input file. 
We argue that computational predictions should not change by ``luck'', and that 
one would prefer full reproducibility. 



\small
\bibliographystyle{plain}

\bibliography{calkan}

\end{document}
