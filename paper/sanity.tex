\documentclass[10pt,a4paper]{article}
\usepackage{graphicx,amssymb}
\usepackage{authblk}
\usepackage{fullpage}
\usepackage{url}

%\usepackage{geometry}
%\geometry{lmargin=0.3in,rmargin=0.3in,tmargin=0in,bmargin=0.1in}

%\textwidth=18cm \hoffset=-0.6cm
%\textheight=28cm \voffset=-1cm


\pagestyle{empty}

\date{}

\def\keywords#1{\begin{center}{\bf Keywords}\\{#1}\end{center}}

\def\titulo#1{\title{#1}}
\def\autores#1{\author{#1}}

\begin{document}

\titulo{On Genomic Repeats and Reproducibility}

\author[1]{Can Firtina}
\author[1]{Can Alkan}
\affil[1]{\small Dept. of Computer Engineering, Bilkent University, 06800 Ankara, Turkey}
%\renewcommand\Authands{ and }

\maketitle
\thispagestyle{empty}

% The abstract

\begin{abstract}

\keywords{genome analysis, reproducibility, repeats}
\end{abstract}

\section{Introduction}
The advancements in high throughput sequencing (HTS) technologies have increased the demand on producing genome sequence data for many research questions, and prompted pilot projects to test its
power in clinical settings~\cite{Biesecker2009}. Any ``medical test'' to be reliably used in the clinic has to be proven to be both accurate and reproducible.
However, the fast-evolving nature of HTS technologies make it difficult to generate essentially same data using the same DNA resources. 

We recently showed that resequencing the same DNA library 
with the same model HTS instrument twice and analyzing the data with the same algorithms leads to different variation call sets~\cite{Kavak2015}. There may be multiple reasons for this effect,
such as degredation of DNA between two sequencing experiments, signal processing and base calling errors during sequencing, or different GC biases introduced while making sequencing libraries from the
same DNA~\cite{Kavak2015}. 

Aside from the potential problems in the ``wet lab'' side as described above~\cite{Kavak2015}, 
there may be additional complications in the ``dry lab'' analysis due to alignment errors and ambiguities due to genomic repeats.
Approximately half of the human genome consists of repeats, which cause ambiguity in read mapping when the read length is short. On the average, a 100 bp read generated by the Illumina platform may align to hundreds of genome locations with similar edit distance. 
The BWA~\cite{Li2009a,Li2013} mapper's  approach to handle such ambiguity is randomly selecting one location, and assigning the mapping quality to zero to inform the variant calling algorithms that the alignment may not be accurate. 

Although many algorithms were developed for HTS data analysis, a handful of computational pipelines from mapping to variant calling may be considered ``standard'' as they are commonly used in large-scale genome projects such as the 1000 Genomes Project~\cite{1000GP,1000GP2012}. For example, to discover and genotype variants using Illumina data, first the reads are mapped to reference genome assembly using BWA~\cite{Li2009a,Li2013} or Bowtie~\cite{Langmead2009}, then the alignment files are processed using SAMtools~\cite{Li2009b} and Picard~\cite{picard}, and finally the single nucleotide polymophisms (SNP) and indels are predicted and filtered using GATK~\cite{DePristo2011}, Platypus~\cite{Rimmer2014}, or Freebayes~\cite{Garrison2012}. 
Structural variation (SV) discovery is even more challenging as exemplified by the 1000 Genomes Project~\cite{1000GP,1000GP2012,Mills2011}, where more than 20 algorithms were used to characterize SVs.
Recently, the Genome in a Bottle Project~\cite{Zook2014} was started to set standards for accurate HTS data analysis for both research and clinical uses by addressing the differences in performances (i.e. accuracy, sensitivity, specificity, etc.) of different algorithms and different sequencing platforms
 to find a ``best practices'' approach.

In this study, we investigated whether some of the commonly used variant discovery algorithms
make use of this mapping quality information, and how they react to genomic repeats.
Briefly, 
we aligned two whole genome shotgun (WGS) datasets with one low (5X) and one high (45X) coverage genome
sequenced as part of the 1000 Genomes Project~\cite{1000GP2012} to the human reference genome (GRCh37) {\bf twice} with the same parameters. 
To test the effects of random placements 
we shuffled the order of reads in the second mapping experiment to make sure that the same random numbers are not used for the same reads. 
In a small scale test, we did not observe any differences
in the alignment files when we used other aligners such as mrFAST~\cite{Alkan2009,Xin2013}, and Bowtie~\cite{Langmead2009}; however, 
BWA reported different map locations to repetitive regions ($\sim$2.8\% of reads) as expected due to its random placement strategy.
We then generated two single nucleotide polymorphism (SNP) and indel callsets each using GATK~\cite{DePristo2011} HaplotypeCaller, GATK UnifiedGenotyper, Freebayes~\cite{Garrison2012}, Platypus~\cite{Rimmer2014}, and SAMTools~\cite{Li2009b}, and structural variation (SV) callsets using DELLY~\cite{Rausch2012} and LUMPY~\cite{Layer2014} from each genome.

We observed substantial differences in the callsets generated by all of the variant discovery tools we tested. GATK's HaplotypeCaller showed a discrepancy of 0.4\% to 1.1\% , 
where UnifiedGenotyper showed the highest number of different calls between two alignments of the same data set (up to 12.76\%). 
Our results raise questions about reproducibility and accuracy of several commonly used genomic variation discovery tools. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Methods}

\subsection{Data acquisition}
We downloaded two whole genome shotgun sequence datasets, one at low coverage ($\sim$5X, HG00096) and one at high coverage ($\sim$ 44X, HG02107) from the 1000 Genomes Project~\cite{1000GP2012}.
We also downloaded 12 whole exome shotgun sequence datasets, again from the 1000 Genomes Project, with coverage ranging from 120X to 656X (Table~\ref{tab:data}). 

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|r|r|r|r|}
\hline
{\bf Sample} & {\bf Type} & {\bf No. reads (mapped)} & {\bf Read length (bp)} & {\bf Coverage$^*$}\\
\hline
HG00096 & WGS & 146,347,388 & 100 & 4.88X\\
HG02107 & WGS & 1,319,393,745 & 101 & 44.42X\\
\hline
HG00250 & WES & 219,687,521 & 76 & 333.93X \\ 
HG00251 & WES & 115,030,162 & 90 & 207.05X \\ 
HG00330 & WES & 108,059,848 & 90 & 194.51X \\ 
HG00731 & WES & 327,901,475 & 76 & 498.41X \\ 
HG00732 & WES & 141,038,355 & 90 & 253.87X \\ 
HG01075 & WES & 431,625,989 & 76 & 656.07X \\ 
HG01083 & WES & 59,336,723 & 101 & 119.86X \\ 
HG01133 & WES & 61,187,487 & 101 & 123.60X \\ 
HG01136 & WES & 185,754,635 & 76 & 282.35X \\ 
HG01140 & WES & 97,205,053 & 90 & 174.97X \\ 
HG01167 & WES & 156,335,569 & 76 & 237.63X \\ 
HG01176 & WES & 63,122,492 & 101 & 127.51X \\ 
\hline
\end{tabular}
\end{center}
\caption{The data sets we downloaded from the 1000 Genomes Project~\cite{1000GP2012} for this study. WGS: whole genome shotgun, WES: whole exome shotgun sequencing. We
  used the WES data sets to test the SNP calls generated by GATK only.
  $^*$Coverage is calculated assuming the genome size as 3 Gbp for WGS data, and the exome size as 50 Mbp for WES data.}
\label{tab:data}
\end{table}


\subsection{Read mapping and shuffling}
We first subsampled 1 million reads from HG00096, and mapped it to the human reference genome (GRCh37) using mrFAST~\cite{Alkan2009,Xin2013}, Bowtie~\cite{Langmead2009}, and BWA~\cite{Li2009a,Li2013}.
Next, we randomly shuffled the reads in the FASTQ file using an in-house program, while keeping the relative order of read pairs intact, and remapped to GRCh37 using the same tools.
After sorting the generated alignment files (i.e. SAM files), we observed that mrFAST and Bowtie generated the same alignments, where BWA mapped some reads to different locations,
as expected from its random placement strategy (data not shown).

We then repeated the same mapping, shuffling, and remapping strategy to the full versions of all datasets we downloaded, but we mapped using only BWA, since the other mappers 
would generate the same alignments before and after reshuffling based on the small scale test. Next, we used Picard~\cite{picard} to remove PCR duplicates (MarkDuplicates).
We then followed the GATK's ``best practices'' guide~\cite{VanderAuwera2013} to
realign around indels (RealignerTargetCreator and IndelRealigner) and recalibrate base quality values (BaseRecalibrator). We used the 
resulting BAM files for SNP, indel, and SV calling. The names and version numbers of the tools we used are listed in Table~\ref{tab:tools}.

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|r|r|}
\hline
{\bf Tool} & {\bf Version} & {\bf Purpose}\\
\hline
mrFAST$^*$~\cite{Alkan2009,Xin2013} & 2.6.1.0 & Read Mapping \\
Bowtie$^*$~\cite{Langmead2009} & 2.2.5 & Read Mapping \\
BWA-MEM~\cite{Li2013} & 0.7.12-r1039 & Read Mapping\\
Picard~\cite{picard} & 1.105 & BAM processing\\
SAMtools~~\cite{Li2009b} & 1.2 & SNPs and indels\\
GATK~\cite{DePristo2011} & 3.4-0-g7e26428 & SNPs and indels\\
Freebayes~\cite{Garrison2012} & 3.4-0-g7e26428 & SNPs and indels\\
Platypus~\cite{Rimmer2014} & 3.4-0-g7e26428 & SNPs and indels\\
DELLY~\cite{Rausch2012} & 0.6.7 & Structural Variation\\
LUMPY~\cite{Layer2014} & 0.2.11 & Structural Variation\\
GASVPro~\cite{Sindi2012} & 1.2 & Structural Variation\\
VariationHunter~\cite{Hormozdiari2010} & 3.0 & Structural Variation\\
\hline
\end{tabular}
\end{center}
\caption{Tools and their version numbers we used in this study. We also list the purpose of each tool. $^*$In small-scale test only.}
\label{tab:tools}
\end{table}

\subsection{SNPs and indels}

We used GATK's~\cite{DePristo2011} both UnifiedGenotyper and HaplotypeCaller algorithms, SAMtools~\cite{Li2009b}, Freebayes~\cite{Garrison2012}, and Platypus~\cite{Rimmer2014} to characterize
SNP and indels. We followed the developer's recommendations and default parameters for all variant calling tools, including potential false positive filters. 
Specifically, we used both Variant Quality Score Recalibrator and SnpCluster algorithms to filter out false positives in GATK call sets, and additionally, 
we required a mapping quality of at least 30 for all tools we tested.

\subsection{Structural variation}

For structural variation discovery using the BWA-generated BAM files, we tested the reproducibility of the calls produced by DELLY~\cite{Rausch2012}, LUMPY~\cite{Layer2014}, and GASVPro~\cite{Sindi2012}. 
We used default parameters for each tool and followed recommendations in relative documentations.

Additionally we also tested VariationHunter~\cite{Hormozdiari2009,Hormozdiari2010}, however, since VariationHunter explicitly requires mrFAST~\cite{Alkan2009,Xin2013} for read mapping, 
both the map location and the calls were fully reproduced. We therefore omit VariationHunter in comparisons in the remainder of the paper.

\subsection{Variant annotation}

We downloaded the coordinates for segmental duplications, genes, coding exons, and common repeats from the UCSC Genome Browser~\cite{Kent2002}. 
We then used the BEDtools suite~\cite{Quinlan2010a} to calculate intersections between discrepancies in call sets with the annotations listed above.

\subsection{Code and script availability} FASTQ read shuffling tool we developed, and all scripts we used to map reads and call variants are available at 
\url{https://github.com/BilkentCompGen/rep_and_rep}.

\section{Results}

As expected, 72 to 80\% of the discrepant calls were found within common repeats. However, we also observed 165 to 4,397 SNVs 
that were called within one alignment but not the other that map to coding exons. 
Furthermore, 691 of the 4397 (15.7\%) discrepant exonic SNVs predicted by GATK UnifiedGenotyper,  did not intersect with any common repeats or segmental duplications.
Freebayes, Platypus, and SAMtools predictions were more reproducable, as $>$98.5\% of the calls were identical.
DELLY also predicted different calls: $\sim$3\% of deletion, $\sim$4\% of tandem duplication, $\sim$6\% of inversion, and $\sim$3.6\% of translocation calls were spefic to a single alignment (i.e. BAM file), and $>$91\% of
these differences intersected with common repeats. More interestingly, when we ran GATK's both HaplotypeCaller and UnifiedGenotyper on the {\bf same} BAM file twice, we observed similar differences. Other tools produced no discrepancies.

As expected, 
Interestingly, although BWA reported zero mapping qualities for 
most of the discrepant read mappings ($\sim$94\%), it also assigned high MAPQ values ($\geq$30) for a fraction of them ($\sim$1.75\%). 


\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|c|c||c|c||c||c|c||c|c||c|}
\hline
{\bf Tool} & \multicolumn{5}{|c||}{\bf HG00096} & \multicolumn{5}{|c|}{\bf HG02107} \\
\hline
{\bf } & \multicolumn{2}{c||}{Original} & \multicolumn{2}{c||}{Shuffled} & {\it Diff (\%) }
& \multicolumn{2}{c||}{Original} & \multicolumn{2}{c||}{Shuffled} & {\it Diff (\%) } \\
\cline{2-11}
{\bf } & {\it All } & {\it Private } & {\it All } & {\it Private } & {\it }
& {\it All } & {\it Private } &  {\it All } & {\it Private } & {\it }\\
\hline
HaplotypeCaller & & & & & & & & & & \\
UnifiedGenotyper & & & & & & & & & & \\
Freebayes & & & & & & & & & & \\
SAMtools & & & & & & & & & & \\
Platypus & & & & & & & & & & \\
\hline
\end{tabular}
\end{center}
\caption{ .. }
\label{tab:snps-orig-vs-shuf}
\end{table}

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|c|c||c|c||c||c|c||c|c||c|}
\hline
{\bf Tool} & \multicolumn{5}{|c||}{\bf HG00096} & \multicolumn{5}{|c|}{\bf HG02107} \\
\hline
{\bf } & \multicolumn{2}{c||}{Original} & \multicolumn{2}{c||}{Shuffled} & {\it Diff (\%) }
& \multicolumn{2}{c||}{Original} & \multicolumn{2}{c||}{Shuffled} & {\it Diff (\%) } \\
\cline{2-11}
{\bf } & {\it All } & {\it Private } & {\it All } & {\it Private } & {\it }
& {\it All } & {\it Private } &  {\it All } & {\it Private } & {\it }\\
\hline
DELLY & & & & & & & & & & \\
LUMPY  & & & & & & & & & & \\
GASVPro & & & & & & & & & & \\
\hline
\end{tabular}
\end{center}
\caption{ .. }
\label{tab:dels-orig-vs-shuf}
\end{table}


\section{Discussion}

The differences in callsets we observed in this study 
may have similar sensitivity and specificity. It is expected to
have differences between different algorithms and/or parameters, but
obtaining different results should not be due to the order of {\it independently generated} reads in the input file. 
We argue that computational predictions should not change by ``luck'', and that 
one would prefer full reproducibility. 



\small
\bibliographystyle{plain}

\bibliography{calkan}

\end{document}
